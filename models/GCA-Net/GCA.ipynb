{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPOzVJaQzpcn"
      },
      "source": [
        "# GCA-Net\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrpMjVsvf2xj"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZi-UDGUzs0b"
      },
      "source": [
        "#conv with 1 in centre of kernal and all  else zero\n",
        "class ShareSepConv(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(ShareSepConv, self).__init__()\n",
        "        assert kernel_size % 2 == 1, 'kernel size should be odd'\n",
        "        self.padding = (kernel_size - 1)//2\n",
        "        weight_tensor = torch.zeros(1, 1, kernel_size, kernel_size)\n",
        "        weight_tensor[0, 0, (kernel_size-1)//2, (kernel_size-1)//2] = 1\n",
        "        self.weight = nn.Parameter(weight_tensor)\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        inc = x.size(1)\n",
        "        expand_weight = self.weight.expand(inc, 1, self.kernel_size, self.kernel_size).contiguous()\n",
        "        return F.conv2d(x, expand_weight,\n",
        "                        None, 1, self.padding, 1, inc)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf2uZXGiBAHL"
      },
      "source": [
        "class SmoothDilatedResidualBlock(nn.Module):\n",
        "    def __init__(self, channel_num, dilation=1, group=1):\n",
        "        super(SmoothDilatedResidualBlock, self).__init__()\n",
        "        self.pre_conv1 = ShareSepConv(dilation*2-1)\n",
        "        self.conv1 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm1 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "        self.pre_conv2 = ShareSepConv(dilation*2-1)\n",
        "        self.conv2 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm2 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.norm1(self.conv1(self.pre_conv1(x))))\n",
        "        y = self.norm2(self.conv2(self.pre_conv2(y)))\n",
        "        return F.relu(x+y)#skip connection"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXJ7O0VtX0FY"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channel_num, dilation=1, group=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm1 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "        self.conv2 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm2 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.norm1(self.conv1(x)))\n",
        "        y = self.norm2(self.conv2(y))\n",
        "        return F.relu(x+y)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHqTjMtbYSSP"
      },
      "source": [
        "class GCANet(nn.Module):\n",
        "    def __init__(self, in_c=4, out_c=3, only_residual=True):\n",
        "        super(GCANet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_c, 64, 3, 1, 1, bias=False)\n",
        "        self.norm1 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, 1, 1, bias=False)\n",
        "        self.norm2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, 2, 1, bias=False)\n",
        "        self.norm3 = nn.InstanceNorm2d(64, affine=True)\n",
        "\n",
        "        self.res1 = SmoothDilatedResidualBlock(64, dilation=2)\n",
        "        self.res2 = SmoothDilatedResidualBlock(64, dilation=2)\n",
        "        self.res3 = SmoothDilatedResidualBlock(64, dilation=2)\n",
        "        self.res4 = SmoothDilatedResidualBlock(64, dilation=4)\n",
        "        self.res5 = SmoothDilatedResidualBlock(64, dilation=4)\n",
        "        self.res6 = SmoothDilatedResidualBlock(64, dilation=4)\n",
        "        self.res7 = ResidualBlock(64, dilation=1)\n",
        "\n",
        "        self.gate = nn.Conv2d(64 * 3, 3, 3, 1, 1, bias=True)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
        "        self.norm4 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
        "        self.norm5 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv1 = nn.Conv2d(64, out_c, 1)\n",
        "        self.only_residual = only_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.norm1(self.conv1(x)))\n",
        "        y = F.relu(self.norm2(self.conv2(y)))\n",
        "        y1 = F.relu(self.norm3(self.conv3(y)))\n",
        "\n",
        "        y = self.res1(y1)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y2 = self.res4(y)\n",
        "        y = self.res5(y2)\n",
        "        y = self.res6(y)\n",
        "        y3 = self.res7(y)\n",
        "\n",
        "        gates = self.gate(torch.cat((y1, y2, y3), dim=1))\n",
        "        gated_y = y1 * gates[:, [0], :, :] + y2 * gates[:, [1], :, :] + y3 * gates[:, [2], :, :]\n",
        "        y = F.relu(self.norm4(self.deconv3(gated_y)))\n",
        "        y = F.relu(self.norm5(self.deconv2(y)))\n",
        "        if self.only_residual:\n",
        "            y = self.deconv1(y)\n",
        "        else:\n",
        "            y = F.relu(self.deconv1(y))\n",
        "\n",
        "        return y"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXzIHsFZ0hg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}