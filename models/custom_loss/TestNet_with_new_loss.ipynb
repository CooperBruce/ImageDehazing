{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestNet_with_new_loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYG0b4CIAInO0BBYTgxC0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakrsiddq/ImageDehazing/blob/main/models/custom_loss/TestNet_with_new_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJt-jM0EGHcW"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from math import log10, sqrt\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEStXK2HGTpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52eaa7c4-d484-42b6-d245-5a5d2773bbab"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qoMTIsGWVg"
      },
      "source": [
        "def load_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels = 3)\n",
        "    img = tf.image.resize(img, size = (413, 550), antialias = True)\n",
        "    img = img / 255.0\n",
        "    return img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7YpgZqsGZ32"
      },
      "source": [
        "def dataset_preposses(orig_path='/content/drive/MyDrive/dataset/clear_images',haze_path='/content/drive/MyDrive/dataset/haze',percentage=0.0005,validation_size=64,test_size=64,seed_val=101):\n",
        "  '''\n",
        "  parameters:\n",
        "  orig_path(string): path of ground truth folder\n",
        "  haze_path(string): path of haze folder\n",
        "  percentage(float): percentage of dataset to load\n",
        "  validation_size(int): the no. of validation images\n",
        "  test_size(int): the no. of test images\n",
        "\n",
        "  returns:\n",
        "  haze_list,validation_list,test_list\n",
        "  '''\n",
        "  random.seed(seed_val)\n",
        "  pth=haze_path+'/*.jpg'\n",
        "  haze_path_list = glob.glob(pth)\n",
        "  orig_path_list=glob.glob(orig_path+'/*.jpg')\n",
        "  #print(orig_path_list)\n",
        "  random.shuffle(haze_path_list)\n",
        "  #print(haze_path_list)\n",
        "  haze_path_dict={}\n",
        "  haze_count_dict={}\n",
        "  haze_list=[]\n",
        "  no_per_set=int(percentage*35)\n",
        "  for i in haze_path_list:\n",
        "    name=i.split('/')[-1].split('_')[0]\n",
        "    if(int(name)>468):\n",
        "      try:\n",
        "        if(haze_count_dict[name]<no_per_set):\n",
        "          haze_path_dict[name].append(i)\n",
        "          \n",
        "          haze_count_dict[name]+=1;\n",
        "          \n",
        "      except KeyError:\n",
        "       \n",
        "        haze_path_dict[name]=[]\n",
        "        haze_path_dict[name].append(i)\n",
        "        haze_count_dict[name]=1\n",
        "    #print(haze_path_dict)\n",
        "  test_list=haze_path_list[-1*test_size:]\n",
        "  val_list=haze_path_list[-1*(validation_size+test_size):-1*test_size];\n",
        "\n",
        "  for (key,val) in haze_path_dict.items():\n",
        "    for i in val:\n",
        "      haze_list.append(i)\n",
        "  return haze_list,val_list,test_list\n",
        "\n",
        "\n",
        "def gen_dataset(ar):\n",
        "  '''\n",
        "  parameters\n",
        "  list of paths\n",
        "  return\n",
        "  list with gt attached \n",
        "  '''\n",
        "  orig_path='/content/drive/MyDrive/dataset/clear_images'\n",
        "  haze_pth='/content/drive/MyDrive/dataset/haze'\n",
        "  lst=[]\n",
        "  for i in ar:\n",
        "    name=i.split('/')[-1].split('_')[0]\n",
        "    pthlist=[i,orig_path+'/'+name+'.jpg']\n",
        "    lst.append(pthlist)\n",
        "  return lst\n",
        "\n",
        "def data_path(orig_img_path = './drive/MyDrive/reside/archive/clear_images', hazy_img_path = './drive/MyDrive/reside/archive/haze'):\n",
        "  \n",
        "  (a,b,c)=dataset_preposses(orig_path=orig_img_path,haze_path=hazy_img_path)\n",
        "  a=gen_dataset(a)\n",
        "  b=gen_dataset(b)\n",
        "  return a,b\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7LkfY76Gfgy"
      },
      "source": [
        "def dataloader(train_data, val_data, batch_size):\n",
        "    print(len(train_data))\n",
        "    train_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in train_data]).map(lambda x: load_image(x))\n",
        "    train_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in train_data]).map(lambda x: load_image(x))\n",
        "    train = tf.data.Dataset.zip((train_data_haze, train_data_orig)).shuffle(buffer_size=100).batch(batch_size)\n",
        "    \n",
        "    val_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in val_data]).map(lambda x: load_image(x))\n",
        "    val_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in val_data]).map(lambda x: load_image(x))\n",
        "    val = tf.data.Dataset.zip((val_data_haze, val_data_orig)).shuffle(buffer_size=100).batch(batch_size)\n",
        "    #print(np.stack(list(train)).shape)\n",
        "    train=np.stack(list(train));val=np.stack(list(val))\n",
        "    return train,val"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bye-i7pGiO-"
      },
      "source": [
        "def display_img(model, hazy_img, orig_img):\n",
        "    \n",
        "    dehazed_img = model(hazy_img, training = True)\n",
        "    plt.figure(figsize = (15,15))\n",
        "    \n",
        "    display_list = [hazy_img[0], orig_img[0], dehazed_img[0]]\n",
        "    title = ['Hazy Image', 'Ground Truth', 'Dehazed Image']\n",
        "    \n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i])\n",
        "        plt.axis('off')\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNTmbcTHGtZC"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8lvxmXiKx0D"
      },
      "source": [
        "# **network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srej_gE4Gtbj"
      },
      "source": [
        "\n",
        "\n",
        "def  custom_Loss():\n",
        "    \n",
        "   \n",
        "    \n",
        "    inputs = tf.keras.Input(shape = [413,550,3])\n",
        "    #pad=tf.keras.layers.ZeroPadding2D(padding=(1, 1))(inputs)\n",
        "\n",
        "    conv1 = Conv2D(input_shape = (413, 550, 3), filters = 3, kernel_size = 1, strides = 1, padding = 'same', use_bias = True,\n",
        "                   kernel_initializer = tf.random_normal_initializer(stddev = 0.02), kernel_regularizer = tf.keras.regularizers.l2(1e-2))(inputs)\n",
        "    \n",
        "    conv2 = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', use_bias = True,\n",
        "                   kernel_initializer = tf.random_normal_initializer(stddev = 0.02), kernel_regularizer = tf.keras.regularizers.l2(1e-2))(conv1)\n",
        "    concat1 = tf.concat([conv1,conv2], axis = -1)\n",
        "    \n",
        "    conv3 = Conv2D(filters = 3, kernel_size = 5, strides = 1, padding = 'same', use_bias = True,\n",
        "                   kernel_initializer = tf.random_normal_initializer(stddev = 0.02), kernel_regularizer = tf.keras.regularizers.l2(1e-2))(concat1)\n",
        "    concat2 = tf.concat([conv2,conv3], axis = -1)\n",
        "    \n",
        "    conv4 = Conv2D(filters = 3, kernel_size = 7, strides = 1, padding = 'same', use_bias = True,\n",
        "                   kernel_initializer = tf.random_normal_initializer(stddev = 0.02), kernel_regularizer = tf.keras.regularizers.l2(1e-2))(concat2)\n",
        "    concat3 = tf.concat([conv1,conv2,conv3,conv4], axis = -1)\n",
        "    \n",
        "    conv5 = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', use_bias = True,\n",
        "                   kernel_initializer = tf.random_normal_initializer(stddev = 0.02), kernel_regularizer = tf.keras.regularizers.l2(1e-2))(concat3)\n",
        "    K = conv5\n",
        "    output = ReLU(max_value = 1.0)(tf.math.multiply(K,inputs) - K + 1.0)\n",
        "    #output1=tf.image.resize_with_crop_or_pad(output1, 412, 548)\n",
        "    #output=tf.image.resize(output1, size = (413, 550), antialias = True)\n",
        "    #model = Model(inputs = x, outputs = output)\n",
        "    \n",
        "    return Model(inputs = inputs, outputs = output)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNT0WSxTLL40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43181568-e487-43a1-bacf-3f4b4c6f9ff7"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "model= custom_Loss()\n",
        "model.build([413,550,3])\n",
        "model.summary()\n",
        "#dot_img_file = '/tmp/model_1.png'\n",
        "#tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 413, 550, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 413, 550, 3)  12          input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 413, 550, 3)  84          conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_9 (TFOpLambda)        (None, 413, 550, 6)  0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 413, 550, 3)  453         tf.concat_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_10 (TFOpLambda)       (None, 413, 550, 6)  0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 413, 550, 3)  885         tf.concat_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_11 (TFOpLambda)       (None, 413, 550, 12) 0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 413, 550, 3)  327         tf.concat_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_3 (TFOpLambda) (None, 413, 550, 3)  0           conv2d_19[0][0]                  \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.subtract_3 (TFOpLambda) (None, 413, 550, 3)  0           tf.math.multiply_3[0][0]         \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_3 (TFOpLam (None, 413, 550, 3)  0           tf.math.subtract_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, 413, 550, 3)  0           tf.__operators__.add_3[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,761\n",
            "Trainable params: 1,761\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I3VLf5aHwNB"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND43J41TLSzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d85555b-fd29-4351-93ec-eee2cdfe982a"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "#train_data, val_data = data_path(orig_img_path = './drive/MyDrive/reside/archive/clear_images', hazy_img_path = './drive/MyDrive/reside/archive/haze')\n",
        "train_data, val_data = data_path(orig_img_path = './drive/MyDrive/dataset/clear_images', hazy_img_path = './drive/MyDrive/dataset/haze')\n",
        "train, val = dataloader(train_data, val_data, batch_size)\n",
        "\n",
        "optimizer = Adam(learning_rate = 1e-4)\n",
        "net = custom_Loss()\n",
        "#net= tf.keras.models.load_model('/content/drive/MyDrive/nets/test_custom_loss_net',compile=False)\n",
        "train_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"train loss\")\n",
        "val_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"val loss\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqftUl4eGtp7",
        "outputId": "c75e9230-19c9-42dc-8ba1-0155d2964f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "a=[];b=[]\n",
        "class MyLoss_layer(tf.keras.losses.Loss):\n",
        "    def __init__(self, from_logits=False,reduction=tf.keras.losses.Reduction.NONE,name='MyLoss_layer'):#=tf.keras.losses.Reduction.AUTO,\n",
        "        super(MyLoss_layer, self).__init__(reduction=reduction, name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        custom_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "        custom_Loss+=1\n",
        "        return custom_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer):\n",
        "    #global a\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print(\"\\nStart of epoch %d\" % (epoch,), end=' ')\n",
        "        start_time_epoch = time.time()\n",
        "        start_time_step = time.time()\n",
        "        \n",
        "        # training loop\n",
        "        \n",
        "        for step, (train_batch_haze, train_batch_orig) in enumerate(train):\n",
        "            #global a\n",
        "            #a=train_batch_haze\n",
        "            #global b\n",
        "            #b=train_batch_orig\n",
        "            #print(type(train_batch_haze))\n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "                train_logits = net(train_batch_haze, training = True)\n",
        "                #loss = mean_squared_error(train_batch_orig, train_logits)\n",
        "                loss = MyLoss_layer(train_batch_orig, train_logits)\n",
        "\n",
        "\n",
        "            grads = tape.gradient(loss, net.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, net.trainable_weights))\n",
        "\n",
        "            train_loss_tracker.update_state(train_batch_orig, train_logits)\n",
        "            if step == 0:\n",
        "                print('[', end='')\n",
        "            if step % 64 == 0:\n",
        "                print('=', end='')\n",
        "        \n",
        "        print(']', end='')\n",
        "        print('  -  ', end='')\n",
        "        print('Training Loss: %.4f' % (train_loss_tracker.result()), end='')\n",
        "        \n",
        "        # validation loop\n",
        "        \n",
        "        for step, (val_batch_haze, val_batch_orig) in enumerate(val):\n",
        "            val_logits = net(val_batch_haze, training = False)\n",
        "            val_loss_tracker.update_state(val_batch_orig, val_logits)\n",
        "            \n",
        "            if step % 32 ==0:\n",
        "                display_img(net, val_batch_haze, val_batch_orig)\n",
        "        \n",
        "        print('  -  ', end='')\n",
        "        print('Validation Loss: %.4f' % (val_loss_tracker.result()), end='')\n",
        "        print('  -  ', end=' ')\n",
        "        print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n",
        "        \n",
        "        net.save('trained_model')           # save the model(variables, weights, etc)\n",
        "        train_loss_tracker.reset_states()\n",
        "        val_loss_tracker.reset_states()\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-23334982ce0b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def call(self, y_true, y_pred):\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxobCF_TGvMW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ba0ad26-12f1-4f6a-d4be-c103fbee67f2"
      },
      "source": [
        "\n",
        "\n",
        "%%time\n",
        "train_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/losses/loss_reduction.py:67: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if key not in cls.all():\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2b67466aa0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-46be7c84d444>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mtrain_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_haze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m#loss = mean_squared_error(train_batch_orig, train_logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLoss_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reduction, name)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/losses/loss_reduction.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid Reduction Key %s.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Invalid Reduction Key [[[[0.5176471  0.69803923 0.8901961 ]\n   [0.5176471  0.69803923 0.8901961 ]\n   [0.5176471  0.69803923 0.8901961 ]\n   ...\n   [0.45882353 0.6156863  0.80784315]\n   [0.45882353 0.6156863  0.80784315]\n   [0.45882353 0.6156863  0.80784315]]\n\n  [[0.52156866 0.7019608  0.89411765]\n   [0.5176471  0.69803923 0.8901961 ]\n   [0.5176471  0.69803923 0.8901961 ]\n   ...\n   [0.46666667 0.6156863  0.80784315]\n   [0.46666667 0.6156863  0.80784315]\n   [0.46666667 0.6156863  0.80784315]]\n\n  [[0.5254902  0.7058824  0.8980392 ]\n   [0.52156866 0.7019608  0.89411765]\n   [0.52156866 0.7019608  0.89411765]\n   ...\n   [0.47058824 0.61960787 0.8117647 ]\n   [0.47058824 0.61960787 0.8117647 ]\n   [0.47058824 0.61960787 0.8117647 ]]\n\n  ...\n\n  [[0.05490196 0.0627451  0.04313726]\n   [0.0627451  0.07058824 0.05098039]\n   [0.05490196 0.0627451  0.05098039]\n   ...\n   [0.         0.02745098 0.01568628]\n   [0.         0.03137255 0.01960784]\n   [0.00392157 0.03921569 0.02745098]]\n\n  [[0.03921569 0.05882353 0.04313726]\n   [0.04705882 0.06666667 0.05098039]\n   [0.05098039 0.07058824 0.05490196]\n   ...\n   [0.00784314 0.03137255 0.01568628]\n   [0.00784314 0.03137255 0.01568628]\n   [0.01176471 0.03529412 0.01960784]]\n\n  [[0.05098039 0.07450981 0.06666667]\n   [0.03529412 0.05882353 0.05098039]\n   [0.03921569 0.05490196 0.05098039]\n   ...\n   [0.03137255 0.06666667 0.0627451 ]\n   [0.03529412 0.06666667 0.07450981]\n   [0.04313726 0.07450981 0.08235294]]]\n\n\n [[[0.80416054 0.81200373 0.80808216]\n   [0.80416054 0.81200373 0.80808216]\n   [0.80416054 0.81200373 0.80808216]\n   ...\n   [0.98431367 0.9882352  0.9647058 ]\n   [0.98431367 0.9882352  0.9647058 ]\n   [0.98431367 0.9882352  0.9647058 ]]\n\n  [[0.80708575 0.8149289  0.8110073 ]\n   [0.80708575 0.8149289  0.8110073 ]\n   [0.80708575 0.8149289  0.8110073 ]\n   ...\n   [0.98431367 0.98823524 0.9647058 ]\n   [0.98431367 0.98823524 0.9647058 ]\n   [0.98431367 0.98823524 0.9647058 ]]\n\n  [[0.81076574 0.8186089  0.8146873 ]\n   [0.81076574 0.8186089  0.8146873 ]\n   [0.8078431  0.8156862  0.81176466]\n   ...\n   [0.9843137  0.98823524 0.9647058 ]\n   [0.9843137  0.98823524 0.9647058 ]\n   [0.9843137  0.98823524 0.9647058 ]]\n\n  ...\n\n  [[0.068863   0.06494144 0.04839255]\n   [0.07072447 0.0668029  0.05025401]\n   [0.07378343 0.06986186 0.05331297]\n   ...\n   [0.04839255 0.04839255 0.04054941]\n   [0.05025401 0.05025401 0.04241088]\n   [0.04925516 0.04925516 0.04141203]]\n\n  [[0.06983095 0.07141597 0.0529764 ]\n   [0.07375252 0.07533754 0.05689796]\n   [0.07767409 0.07925911 0.06081954]\n   ...\n   [0.05696841 0.05696841 0.04912527]\n   [0.05813669 0.05813669 0.05029355]\n   [0.05696841 0.05696841 0.04912527]]\n\n  [[0.47527647 0.47919804 0.4595902 ]\n   [0.47719625 0.4811178  0.46150997]\n   [0.479116   0.48303756 0.46342972]\n   ...\n   [0.46807566 0.46807566 0.46023253]\n   [0.47007748 0.47007748 0.46223435]\n   [0.47007748 0.47007748 0.46223435]]]\n\n\n [[[0.8509802  0.85490185 0.8745097 ]\n   [0.8509802  0.85490185 0.8745097 ]\n   [0.8509802  0.85490185 0.8745097 ]\n   ...\n   [0.83483934 0.8505256  0.8975509 ]\n   [0.8348057  0.85049206 0.8975173 ]\n   [0.8348057  0.85049206 0.8975173 ]]\n\n  [[0.85220796 0.85612947 0.8757373 ]\n   [0.85220796 0.85612947 0.8757373 ]\n   [0.85220796 0.85612947 0.8757373 ]\n   ...\n   [0.84153074 0.857217   0.9009227 ]\n   [0.8389982  0.8546845  0.8983902 ]\n   [0.8377706  0.85345685 0.8971626 ]]\n\n  [[0.8530824  0.85700405 0.8766119 ]\n   [0.8525329  0.8564545  0.87606233]\n   [0.8525329  0.8564545  0.87606233]\n   ...\n   [0.83808696 0.8537732  0.8904489 ]\n   [0.8373962  0.8530824  0.8897581 ]\n   [0.83584356 0.85152984 0.8882055 ]]\n\n  ...\n\n  [[0.44202277 0.4616306  0.43417963]\n   [0.43051085 0.45011872 0.4226677 ]\n   [0.42347005 0.44307792 0.4156269 ]\n   ...\n   [0.5169717  0.5169717  0.47775605]\n   [0.51305014 0.51305014 0.47383446]\n   [0.51163894 0.51163894 0.47132415]]\n\n  [[0.4487639  0.46837175 0.44092077]\n   [0.44255537 0.4621632  0.43471226]\n   [0.43441743 0.45402524 0.42657426]\n   ...\n   [0.52118975 0.52118975 0.48197404]\n   [0.51726824 0.51726824 0.4780525 ]\n   [0.51451814 0.51451814 0.47530246]]\n\n  [[0.4466717  0.46627954 0.43882856]\n   [0.44075567 0.4603635  0.43291253]\n   [0.43614408 0.4557519  0.42830092]\n   ...\n   [0.51327103 0.51327103 0.4740553 ]\n   [0.5107212  0.5107212  0.47150543]\n   [0.50983757 0.50983757 0.47062185]]]\n\n\n [[[0.8156863  0.8509804  0.8784314 ]\n   [0.8156863  0.8509804  0.8784314 ]\n   [0.8156863  0.8509804  0.8784314 ]\n   ...\n   [0.8901961  0.89411765 0.9019608 ]\n   [0.8901961  0.89411765 0.9019608 ]\n   [0.8901961  0.89411765 0.9019608 ]]\n\n  [[0.8156863  0.8509804  0.8784314 ]\n   [0.8156863  0.8509804  0.8784314 ]\n   [0.8156863  0.8509804  0.8784314 ]\n   ...\n   [0.8901961  0.89411765 0.9019608 ]\n   [0.8901961  0.89411765 0.9019608 ]\n   [0.8901961  0.89411765 0.9019608 ]]\n\n  [[0.8117647  0.84705883 0.8745098 ]\n   [0.8117647  0.84705883 0.8745098 ]\n   [0.8156863  0.8509804  0.8784314 ]\n   ...\n   [0.8901961  0.89411765 0.9019608 ]\n   [0.8901961  0.89411765 0.9019608 ]\n   [0.8901961  0.89411765 0.9019608 ]]\n\n  ...\n\n  [[0.10588235 0.14509805 0.11372549]\n   [0.11764706 0.15686275 0.1254902 ]\n   [0.12941177 0.16862746 0.13725491]\n   ...\n   [0.3137255  0.2784314  0.24313726]\n   [0.30980393 0.27450982 0.24705882]\n   [0.2901961  0.25490198 0.22745098]]\n\n  [[0.10588235 0.13333334 0.10588235]\n   [0.11372549 0.14117648 0.11372549]\n   [0.1254902  0.15294118 0.1254902 ]\n   ...\n   [0.3137255  0.2784314  0.24313726]\n   [0.30980393 0.28235295 0.2509804 ]\n   [0.29411766 0.26666668 0.23529412]]\n\n  [[0.10980392 0.12941177 0.10588235]\n   [0.11372549 0.13333334 0.10980392]\n   [0.11372549 0.13333334 0.11764706]\n   ...\n   [0.28235295 0.27450982 0.22745098]\n   [0.27058825 0.27058825 0.23137255]\n   [0.2509804  0.2509804  0.21176471]]]]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmY_-owraQvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a36314-e1a4-4986-a7c3-8ecd7b6122a9"
      },
      "source": [
        "%%time\n",
        "b=a.numpy()\n",
        "#\n",
        "k=0\n",
        "for i in range(b.shape[0]):\n",
        "  atomsphericLight = getAtomsphericLight(b[i,:,:,:])\n",
        "  k=k+atomsphericLight\n",
        "  print(atomsphericLight)\n",
        "print(k/b.shape[0])  \n",
        "#b.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.83746743\n",
            "0.91764706\n",
            "0.99215686\n",
            "0.9137255\n",
            "0.9152492135763168\n",
            "CPU times: user 23.2 s, sys: 114 ms, total: 23.3 s\n",
            "Wall time: 23.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6DS2JJKG1nr"
      },
      "source": [
        "net.save('./drive/MyDrive/nets/test_custom_loss_net')\n",
        "model=net\n",
        "#model.build([412,548,3])\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XxulTP8G6vZ"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def msee(imageA, imageB):\n",
        "\t# the 'Mean Squared Error' between the two images is the\n",
        "\t# sum of the squared difference between the two images;\n",
        "\t# NOTE: the two images must have the same dimension\n",
        "\terr = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        "\terr /= float(imageA.shape[0] * imageA.shape[1])\n",
        "\t\n",
        "\t# return the MSE, the lower the error, the more \"similar\"\n",
        "\t# the two images are\n",
        "\treturn err\n",
        "def compare_images(imageA, imageB):\n",
        "  # compute the mean squared error and structural similarity\n",
        "  # index for the images\n",
        "  m = msee(imageA, imageB)\n",
        "  s = ssim(imageA, imageB,multichannel=True)\n",
        "  return s\n",
        "  #s=tf.image.ssim(imageA, imageB, max_val=255, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)\n",
        "def find_psnr(imageA,imageB):\n",
        "   return cv2.PSNR(imageA,imageB)\n",
        "\n",
        "def evaluate_gen(net):\n",
        "    \n",
        "    #test_img = glob.glob(test_img_path +'/*.jpg')\n",
        "    test_img=glob.glob('/content/drive/MyDrive/Final_compare/HAZY/*.jpg')\n",
        "    \n",
        "    #random.shuffle(test_img)\n",
        "    i=1;\n",
        "    for img in test_img:\n",
        "        \n",
        "        img = tf.io.read_file(img)\n",
        "        img = tf.io.decode_jpeg(img, channels = 3)\n",
        "        \n",
        "        img = tf.image.resize(img, size = (412,548), antialias = True)\n",
        "        \n",
        "        img = img / 255.0\n",
        "        print(i,end=\" \")\n",
        "        img = tf.expand_dims(img, axis = 0)      #transform input image from 3D to 4D ###\n",
        "        \n",
        "        dehaze = net(img)\n",
        "        dehaze=tf.image.resize(dehaze, size = (413,550), antialias = True)\n",
        "        #plt.figure(figsize = (80, 80))\n",
        "        \n",
        "        #display_list = [img[0], dehaze[0]]       #make the first dimension zero\n",
        "        im=dehaze[0]\n",
        "        directory = '/content/drive/MyDrive/Final_compare/test_custom_loss_net'\n",
        "        \n",
        "        os.chdir(directory)\n",
        "        filename = str(i) + '_outdoor_gen.jpg'\n",
        "        #print(filename)\n",
        "        #cv2.imwrite(filename,im) \n",
        "        #plt.imsave(filename,im)\n",
        "        tf.keras.preprocessing.image.save_img(filename,im)\n",
        "\n",
        "        os.chdir('/content')\n",
        "        i+=1;\n",
        "    orig_img_path='/content/drive/MyDrive/Final_compare/GT'\n",
        "    orig_img = glob.glob(orig_img_path + '/*.jpg')\n",
        "    orig_img.sort()\n",
        "\n",
        "    test_img_path='/content/drive/MyDrive/Final_compare/test_custom_loss_net'\n",
        "    \n",
        "    test_img = glob.glob(test_img_path + '/*.jpg')\n",
        "    test_img.sort()\n",
        "\n",
        "\n",
        "    psnr_val=0;ssim_val=0;\n",
        "    n=len(orig_img)\n",
        "    for i in range(len(orig_img)):\n",
        "      os.chdir(orig_img_path)\n",
        "      original = cv2.imread(orig_img[i]) \n",
        "      #original=tf.image.resize_with_crop_or_pad(original, 400, 520)\n",
        "      original = original[3:-4,3:-4,]\n",
        "      os.chdir(test_img_path)\n",
        "      hazy = cv2.imread(test_img[i]) \n",
        "      #hazy=tf.image.resize_with_crop_or_pad(hazy, 400, 520)\n",
        "      hazy=hazy[3:-4,3:-4]\n",
        "      psnr_val+=find_psnr(original, hazy)\n",
        "      ssim_val+=compare_images(original, hazy)\n",
        "      #print(i,end=\" \")\n",
        "    total_psnr=psnr_val/n\n",
        "    total_ssim=ssim_val/n\n",
        "    print(\"\\n psnr=\",total_psnr)\n",
        "    print(\"ssim=\",total_ssim)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDIcNhLTHIQP"
      },
      "source": [
        "\n",
        "\n",
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/nets/test_custom_loss_net',compile=False)\n",
        "\n",
        "\n",
        "evaluate_gen(new_model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP10YhKaHVB9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}